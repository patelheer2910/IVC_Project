{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c74a21-6b77-4e37-9656-dd3289bc61a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dlib in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (19.24.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90922170-ec70-45af-a812-bed6d99dac6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mediapipe in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (0.10.21)\n",
      "Requirement already satisfied: absl-py in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (3.9.1)\n",
      "Requirement already satisfied: numpy<2 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from jax->mediapipe) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (from jax->mediapipe) (1.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (2.9.0)\n",
      "Requirement already satisfied: pycparser in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f172e3f-6481-43de-884e-82ef0612ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c05b6b6-e1c6-4bbe-bc70-82d452a7f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # Download from dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfc86cf8-0da1-4f77-9777-28f5c6b8ccaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the face image and the mask\n",
    "face_img = cv2.imread(\"face.jpg\")\n",
    "# face_img = cv2.imread(\"/projectnb/cs585bp/students/dlgirija/colorferet_extracted/00002/00002_930831_fa.ppm\")\n",
    "mask_img = cv2.imread(\"Surgical-Mask.png\", cv2.IMREAD_UNCHANGED)  # Ensure mask has an alpha channel\n",
    "\n",
    "# Convert to grayscale and detect faces\n",
    "gray = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
    "faces = detector(gray)\n",
    "\n",
    "for face in faces:\n",
    "    landmarks = predictor(gray, face)\n",
    "    \n",
    "    # Get coordinates for mask placement (adjust as needed)\n",
    "    top_nose = (landmarks.part(19).x, landmarks.part(19).y)\n",
    "    chin = (landmarks.part(57).x, landmarks.part(57).y)\n",
    "    left_cheek = (landmarks.part(0).x, landmarks.part(0).y)\n",
    "    right_cheek = (landmarks.part(16).x, landmarks.part(16).y)\n",
    "\n",
    "    # Calculate mask size\n",
    "    mask_width = int(np.linalg.norm(np.array(left_cheek) - np.array(right_cheek)))\n",
    "    mask_height = int(np.linalg.norm(np.array(top_nose) - np.array(chin))) * 2  # Scale factor\n",
    "\n",
    "    # Resize the mask\n",
    "    mask_resized = cv2.resize(mask_img, (mask_width, mask_height))\n",
    "\n",
    "    # Get region of interest (ROI) in face image\n",
    "    x1, y1 = left_cheek[0], top_nose[1]\n",
    "    x2, y2 = x1 + mask_width, y1 + mask_height\n",
    "    roi = face_img[y1:y2, x1:x2]\n",
    "\n",
    "    # Extract mask alpha channel for blending\n",
    "    mask_alpha = mask_resized[:, :, 3] / 255.0\n",
    "    mask_rgb = mask_resized[:, :, :3]\n",
    "    mask_rgb_resized = cv2.resize(mask_rgb, (roi.shape[1], roi.shape[0]))\n",
    "    mask_alpha_resized = cv2.resize(mask_alpha, (roi.shape[1], roi.shape[0]))  # (Width, Height)\n",
    "    \n",
    "\n",
    "    # Blend the mask with the face\n",
    "    for c in range(3):\n",
    "        # print(1-mask_alpha)\n",
    "        roi[:, :, c] = roi[:, :, c] * (1 - mask_alpha_resized) + mask_rgb_resized[:, :, c] * mask_alpha_resized\n",
    "\n",
    "    # Place the modified ROI back into the image\n",
    "    face_img[y1:y2, x1:x2] = roi\n",
    "\n",
    "# Show the output\n",
    "# cv2.imshow(\"Masked Face\", face_img)\n",
    "cv2.imwrite(\"output.jpg\", face_img)  # Saves 'image' as \"output.jpg\"\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4b08fd5-889f-4e53-927a-61b9dd8e2aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bz2\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Decompress .ppm.bz2 file\n",
    "with bz2.BZ2File(\"/projectnb/cs585bp/students/dlgirija/colorferet/images/00001/00001_930831_fb_a.ppm.bz2\", \"rb\") as f:\n",
    "    decompressed_data = f.read()\n",
    "\n",
    "# Convert the decompressed data into a numpy array\n",
    "ppm_array = np.frombuffer(decompressed_data, dtype=np.uint8)\n",
    "\n",
    "# Decode the PPM image using OpenCV\n",
    "img = cv2.imdecode(ppm_array, cv2.IMREAD_COLOR)\n",
    "cv2.imwrite(\"detectos.jpg\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73534ee8-bade-4f00-815a-fea07b1f8154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Open image using PIL\n",
    "# img = Image.open(\"image.ppm\")\n",
    "\n",
    "# Convert to OpenCV format (NumPy array)\n",
    "# img_cv2 = np.array(img)\n",
    "face_img = cv2.imread(\"face.jpg\")\n",
    "# face_img = cv2.imread(\"/projectnb/cs585bp/students/dlgirija/colorferet_extracted/00001/00001_930831_fa_a.ppm\")\n",
    "# img = Image.open(\"/projectnb/cs585bp/students/dlgirija/colorferet_extracted/00001/00001_930831_fa_a.ppm\")\n",
    "# face_img = np.array(img)\n",
    "for i in range(68):  # 68 landmarks for dlib\n",
    "    x, y = landmarks.part(i).x, landmarks.part(i).y\n",
    "    cv2.circle(face_img, (x, y), 2, (0, 0, 255), -1)\n",
    "cv2.imwrite(\"detectos.jpg\", face_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f4436-f241-4be7-931b-2680f6523e6f",
   "metadata": {},
   "source": [
    "# MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063f9bd7-31a6-4c94-bec3-fefb0edd6af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740165668.129923   90905 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740165668.139516   90907 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# cap = cv2.imread('face.jpg')\n",
    "cap = cv2.imread(\"/projectnb/cs585bp/students/dlgirija/colorferet_extracted/00001/00001_930831_hr_a.ppm\")\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    frame = cap\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "    if results.multi_face_landmarks:\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            # cv2.putText(frame, str(idx), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "            mp_drawing.draw_landmarks(frame, landmarks, mp_face_mesh.FACEMESH_CONTOURS)\n",
    "\n",
    "    cv2.imwrite('detectos.jpg', frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d1be9e4a-83ff-4576-b2fe-127f8a6b1f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740162945.945170 1391524 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740162945.964192 1391527 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load the image\n",
    "image_path = \"face.jpg\"  # Replace with your image path\n",
    "image = cv2.imread(image_path)\n",
    "h, w, _ = image.shape  # Get image dimensions\n",
    "\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "results = face_mesh.process(image_rgb)\n",
    "\n",
    "if results.multi_face_landmarks:\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        for idx, landmark in enumerate(face_landmarks.landmark):\n",
    "            x, y = int(landmark.x * w), int(landmark.y * h)  # Convert to image coordinates\n",
    "            \n",
    "            cv2.putText(image, str(idx), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "            cv2.circle(image, (x, y), 1, (255, 0, 0), -1)  # Draw small dot on each landmark\n",
    "\n",
    "# Display the result\n",
    "cv2.imwrite('Detectors.jpg', image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7b0cd18-9ff1-4f59-948d-a4aabdd092bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740168518.202766   96009 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740168518.220213   96018 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "### import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Load image\n",
    "# cap = cv2.imread(\"/projectnb/cs585bp/students/dlgirija/colorferet_extracted/00001/00001_930831_fa_a.ppm\")\n",
    "cap = cv2.imread('face.jpg')\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    frame = cap\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            # Define the landmark indices for the mouth and nose region\n",
    "            MOUTH_INDICES = [\n",
    "                93, 137, 123, 50, 101, 100, 47, 114, 122, 188, 6, # left\n",
    "                351, 412, 343, 277, 329, 330, 280, 352, 366,323, # right\n",
    "            ]\n",
    "            NOSE_INDICES = []\n",
    "            # Get landmark coordinates for mask area\n",
    "            h, w, _ = frame.shape\n",
    "            mask_points = []\n",
    "            frame_points = []\n",
    "            for idx in MOUTH_INDICES + NOSE_INDICES:\n",
    "                x = int(landmarks.landmark[idx].x * w)\n",
    "                y = int(landmarks.landmark[idx].y * h)\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "\n",
    "            left_ind = [132,215,172,136,150,149,176,148,152] # left\n",
    "            right_ind = [377,400,378,379,365,397,288,361] # right\n",
    "\n",
    "            for idx in left_ind:\n",
    "                x = int(landmarks.landmark[idx].x * w) - 15\n",
    "                y = int(landmarks.landmark[idx].y * h) + 10\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "\n",
    "            for idx in right_ind:\n",
    "                x = int(landmarks.landmark[idx].x * w) + 15\n",
    "                y = int(landmarks.landmark[idx].y * h) + 10\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "\n",
    "            mask = frame\n",
    "            hull = cv2.convexHull(np.array(mask_points, np.int32))\n",
    "            cv2.fillConvexPoly(mask, hull, (100, 100, 100))\n",
    "            frame = mask\n",
    "            \n",
    "    cv2.imwrite(\"Masked_Image.jpg\", frame)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d609ae10-cb65-4e50-9030-e31eebb41c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 132,#, 377\n",
    "                # 377,\n",
    "                \n",
    "                # 51, 174, 217, 126, 100, 101, 50, 123, 132,\n",
    "                # 114,3, 47, 100, 101, 234, 50, 123,\n",
    "                # 281, 363, 420, 355, 329, 330, 280, 352,#347, 346, 345,\n",
    "                # 197, 419, 399, 343, 350, 451, 450, 448, 449, 261, 265\n",
    "                # 401, 433, 435, 288, 367, 379, 401, 323, 366, 376, 454, 425, 426, 436, 427, \n",
    "                # 61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 78, 191, 80, 81, 82, \n",
    "                # 13, 312, 311, 310, 415, 308, 324, 318, 402, 317, 14, 87, 377, 152, 400, 148, 213, 214, 192, 138, 32, 74, 397, 365\n",
    "\n",
    "# MOUTH_INDICES += [i for i in range()]\n",
    "            # NOSE_INDICES = [2, 98, 327, 168]  # Nose bridge and tip\n",
    "            # NOSE_INDICES = [197]#, 5, 4, 1] \n",
    "\n",
    "# cv2.fillPoly(mask, [np.array(mask_points, np.int32)], (100, 100, 100))  # Black-out mask area\n",
    "            # for i in range(len(mask_points)):\n",
    "            #     cv2.putText(mask, str(frame_points[i]), mask_points[i], cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "            #     cv2.circle(mask, mask_points[i], 2, (0, 0, 255), -1)\n",
    "            # Apply mask\n",
    "\n",
    "# Create mask and fill region\n",
    "            # mask = np.ones_like(frame) * 255  # White mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d91bfeb9-3822-4981-a9d0-572a08b293bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 1372, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5140b7a6-9162-4f98-b696-c386ca56e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "texture =cv2.imread('grass_texture.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4f44a0f-6c45-429c-8d46-ec915f625c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 1372, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texture[0: mask.shape[0], 0:mask.shape[1], :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9925bf77-dc78-4329-9842-3613f90bd823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740171015.454991   99504 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740171015.463138   99504 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Load image\n",
    "cap = cv2.imread('face.jpg')\n",
    "texture = cv2.imread('grass_texture.jpeg')\n",
    "\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    frame = cap\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            # Define the landmark indices for the mouth and nose region\n",
    "            MOUTH_INDICES = [\n",
    "                93, 137, 123, 50, 101, 100, 47, 114, 122, 188, 6, # left\n",
    "                351, 412, 343, 277, 329, 330, 280, 352, 366,323, # right\n",
    "            ]\n",
    "            NOSE_INDICES = []\n",
    "            \n",
    "            # Get landmark coordinates for mask area\n",
    "            h, w, _ = frame.shape\n",
    "            mask_points = []\n",
    "            for idx in MOUTH_INDICES + NOSE_INDICES:\n",
    "                x = int(landmarks.landmark[idx].x * w)\n",
    "                y = int(landmarks.landmark[idx].y * h)\n",
    "                mask_points.append((x, y))\n",
    "\n",
    "            left_ind = [132, 215, 172, 136, 150, 149, 176, 148, 152]  # left\n",
    "            right_ind = [377, 400, 378, 379, 365, 397, 288, 361]  # right\n",
    "\n",
    "            for idx in left_ind:\n",
    "                x = int(landmarks.landmark[idx].x * w)# - 15\n",
    "                y = int(landmarks.landmark[idx].y * h)# + 10\n",
    "                mask_points.append((x, y))\n",
    "\n",
    "            for idx in right_ind:\n",
    "                x = int(landmarks.landmark[idx].x * w)# + 15\n",
    "                y = int(landmarks.landmark[idx].y * h)# + 10\n",
    "                mask_points.append((x, y))\n",
    "\n",
    "            mask = frame\n",
    "            texture = cv2.resize(texture, (mask.shape[1], mask.shape[0]))  # Resize texture to match the mask size\n",
    "            # print(texture.shape)\n",
    "            hull = cv2.convexHull(np.array(mask_points, np.int32))\n",
    "\n",
    "            # Create a single-channel mask (grayscale) for the convex hull area\n",
    "            hull_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.uint8)  # Single-channel mask\n",
    "            cv2.fillConvexPoly(hull_mask, hull, 255)  # Fill the hull with white (255) in single-channel mask\n",
    "\n",
    "            # Apply texture using the hull mask (only inside the convex hull)\n",
    "            texture_in_hull = cv2.bitwise_and(texture, texture, mask=hull_mask)\n",
    "\n",
    "            # Keep the original image outside the hull and blend the texture inside the hull\n",
    "            mask = cv2.bitwise_and(mask, mask, mask=cv2.bitwise_not(hull_mask))  # Keep outside the hull\n",
    "            mask = cv2.add(mask, texture_in_hull)  # Add texture inside the hull\n",
    "\n",
    "            frame = mask\n",
    "\n",
    "    # Save the resulting image with texture inside the hull\n",
    "    cv2.imwrite(\"Masked_Image.jpg\", frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03be20-0790-4138-93ef-89cfc972f32f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
