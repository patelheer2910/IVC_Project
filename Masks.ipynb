{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c435fbc6-fe6e-4587-ba2c-c71625ff383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dlib in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (19.24.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a3567f5-d46f-426d-8868-3b500522f491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mediapipe in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (0.10.21)\n",
      "Requirement already satisfied: absl-py in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (3.9.1)\n",
      "Requirement already satisfied: numpy<2 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from jax->mediapipe) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /usr3/graduate/dlgirija/.local/lib/python3.11/site-packages (from jax->mediapipe) (1.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib->mediapipe) (2.9.0)\n",
      "Requirement already satisfied: pycparser in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49971af7-8fd1-4790-b06a-15cd6ad94e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f741f26-dfa0-464b-9af2-20805e63644d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9221496-2a63-48b4-8d16-4fa19c65c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # Download from dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c04e69fa-4a61-47f1-8f65-df5c3b9f3b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the face image and the mask\n",
    "face_img = cv2.imread(\"face.jpg\")\n",
    "# face_img = cv2.imread(\"/projectnb/cs585bp/students/dlgirija/colorferet_extracted/00002/00002_930831_fa.ppm\")\n",
    "mask_img = cv2.imread(\"Surgical-Mask.png\", cv2.IMREAD_UNCHANGED)  # Ensure mask has an alpha channel\n",
    "\n",
    "# Convert to grayscale and detect faces\n",
    "gray = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
    "faces = detector(gray)\n",
    "\n",
    "for face in faces:\n",
    "    landmarks = predictor(gray, face)\n",
    "    \n",
    "    # Get coordinates for mask placement (adjust as needed)\n",
    "    top_nose = (landmarks.part(19).x, landmarks.part(19).y)\n",
    "    chin = (landmarks.part(57).x, landmarks.part(57).y)\n",
    "    left_cheek = (landmarks.part(0).x, landmarks.part(0).y)\n",
    "    right_cheek = (landmarks.part(16).x, landmarks.part(16).y)\n",
    "\n",
    "    # Calculate mask size\n",
    "    mask_width = int(np.linalg.norm(np.array(left_cheek) - np.array(right_cheek)))\n",
    "    mask_height = int(np.linalg.norm(np.array(top_nose) - np.array(chin))) * 2  # Scale factor\n",
    "\n",
    "    # Resize the mask\n",
    "    mask_resized = cv2.resize(mask_img, (mask_width, mask_height))\n",
    "\n",
    "    # Get region of interest (ROI) in face image\n",
    "    x1, y1 = left_cheek[0], top_nose[1]\n",
    "    x2, y2 = x1 + mask_width, y1 + mask_height\n",
    "    roi = face_img[y1:y2, x1:x2]\n",
    "\n",
    "    # Extract mask alpha channel for blending\n",
    "    mask_alpha = mask_resized[:, :, 3] / 255.0\n",
    "    mask_rgb = mask_resized[:, :, :3]\n",
    "    mask_rgb_resized = cv2.resize(mask_rgb, (roi.shape[1], roi.shape[0]))\n",
    "    mask_alpha_resized = cv2.resize(mask_alpha, (roi.shape[1], roi.shape[0]))  # (Width, Height)\n",
    "    \n",
    "\n",
    "    # Blend the mask with the face\n",
    "    for c in range(3):\n",
    "        # print(1-mask_alpha)\n",
    "        roi[:, :, c] = roi[:, :, c] * (1 - mask_alpha_resized) + mask_rgb_resized[:, :, c] * mask_alpha_resized\n",
    "\n",
    "    # Place the modified ROI back into the image\n",
    "    face_img[y1:y2, x1:x2] = roi\n",
    "\n",
    "# Show the output\n",
    "# cv2.imshow(\"Masked Face\", face_img)\n",
    "cv2.imwrite(\"output.jpg\", face_img)  # Saves 'image' as \"output.jpg\"\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1d9d154-0f7e-4172-9238-18a1985b1301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bz2\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Decompress .ppm.bz2 file\n",
    "with bz2.BZ2File(\"/projectnb/cs585bp/students/dlgirija/colorferet/images/00001/00001_930831_fb_a.ppm.bz2\", \"rb\") as f:\n",
    "    decompressed_data = f.read()\n",
    "\n",
    "# Convert the decompressed data into a numpy array\n",
    "ppm_array = np.frombuffer(decompressed_data, dtype=np.uint8)\n",
    "\n",
    "# Decode the PPM image using OpenCV\n",
    "img = cv2.imdecode(ppm_array, cv2.IMREAD_COLOR)\n",
    "cv2.imwrite(\"detectos.jpg\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e0675b9-6874-46a2-97da-d32d153bcd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Open image using PIL\n",
    "# img = Image.open(\"image.ppm\")\n",
    "\n",
    "# Convert to OpenCV format (NumPy array)\n",
    "# img_cv2 = np.array(img)\n",
    "face_img = cv2.imread(\"face.jpg\")\n",
    "# face_img = cv2.imread(\"/projectnb/cs585bp/students/dlgirija/colorferet_extracted/00001/00001_930831_fa_a.ppm\")\n",
    "# img = Image.open(\"/projectnb/cs585bp/students/dlgirija/colorferet_extracted/00001/00001_930831_fa_a.ppm\")\n",
    "# face_img = np.array(img)\n",
    "for i in range(68):  # 68 landmarks for dlib\n",
    "    x, y = landmarks.part(i).x, landmarks.part(i).y\n",
    "    cv2.circle(face_img, (x, y), 2, (0, 0, 255), -1)\n",
    "cv2.imwrite(\"detectos.jpg\", face_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09abdd9-ea1f-48de-9f01-6de4e7815284",
   "metadata": {},
   "source": [
    "# MediaPipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad901612-8962-4a95-98fd-ae30ba8463af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15163728-be73-4d7c-9101-4fc9928177dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740165668.129923   90905 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740165668.139516   90907 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# cap = cv2.imread('face.jpg')\n",
    "cap = cv2.imread(\"/projectnb/cs585bp/students/dlgirija/colorferet_extracted/00001/00001_930831_hr_a.ppm\")\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    frame = cap\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "    if results.multi_face_landmarks:\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            # cv2.putText(frame, str(idx), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "            mp_drawing.draw_landmarks(frame, landmarks, mp_face_mesh.FACEMESH_CONTOURS)\n",
    "\n",
    "    cv2.imwrite('detectos.jpg', frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "07c308f7-916f-4318-a283-0d66df186d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740162945.945170 1391524 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740162945.964192 1391527 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load the image\n",
    "image_path = \"face.jpg\"  # Replace with your image path\n",
    "image = cv2.imread(image_path)\n",
    "h, w, _ = image.shape  # Get image dimensions\n",
    "\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "results = face_mesh.process(image_rgb)\n",
    "\n",
    "if results.multi_face_landmarks:\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        for idx, landmark in enumerate(face_landmarks.landmark):\n",
    "            x, y = int(landmark.x * w), int(landmark.y * h)  # Convert to image coordinates\n",
    "            \n",
    "            cv2.putText(image, str(idx), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "            cv2.circle(image, (x, y), 1, (255, 0, 0), -1)  # Draw small dot on each landmark\n",
    "\n",
    "# Display the result\n",
    "cv2.imwrite('Detectors.jpg', image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fee380-cad8-4b23-a995-c7cda15228b9",
   "metadata": {},
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b962fe06-d6b0-46ea-a520-1d1ea4e53b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740168518.202766   96009 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740168518.220213   96018 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "### import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Load image\n",
    "# cap = cv2.imread(\"/projectnb/cs585bp/students/dlgirija/colorferet_extracted/00001/00001_930831_fa_a.ppm\")\n",
    "cap = cv2.imread('face.jpg')\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    frame = cap\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            # Define the landmark indices for the mouth and nose region\n",
    "            MOUTH_INDICES = [\n",
    "                93, 137, 123, 50, 101, 100, 47, 114, 122, 188, 6, # left\n",
    "                351, 412, 343, 277, 329, 330, 280, 352, 366,323, # right\n",
    "            ]\n",
    "            NOSE_INDICES = []\n",
    "            # Get landmark coordinates for mask area\n",
    "            h, w, _ = frame.shape\n",
    "            mask_points = []\n",
    "            frame_points = []\n",
    "            for idx in MOUTH_INDICES + NOSE_INDICES:\n",
    "                x = int(landmarks.landmark[idx].x * w)\n",
    "                y = int(landmarks.landmark[idx].y * h)\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "\n",
    "            left_ind = [132,215,172,136,150,149,176,148,152] # left\n",
    "            right_ind = [377,400,378,379,365,397,288,361] # right\n",
    "\n",
    "            for idx in left_ind:\n",
    "                x = int(landmarks.landmark[idx].x * w) - 15\n",
    "                y = int(landmarks.landmark[idx].y * h) + 10\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "\n",
    "            for idx in right_ind:\n",
    "                x = int(landmarks.landmark[idx].x * w) + 15\n",
    "                y = int(landmarks.landmark[idx].y * h) + 10\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "\n",
    "            mask = frame\n",
    "            hull = cv2.convexHull(np.array(mask_points, np.int32))\n",
    "            cv2.fillConvexPoly(mask, hull, (100, 100, 100))\n",
    "            frame = mask\n",
    "            \n",
    "    cv2.imwrite(\"Masked_Image.jpg\", frame)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed0ce592-1961-4c40-bc0a-7fd664366ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 132,#, 377\n",
    "                # 377,\n",
    "                \n",
    "                # 51, 174, 217, 126, 100, 101, 50, 123, 132,\n",
    "                # 114,3, 47, 100, 101, 234, 50, 123,\n",
    "                # 281, 363, 420, 355, 329, 330, 280, 352,#347, 346, 345,\n",
    "                # 197, 419, 399, 343, 350, 451, 450, 448, 449, 261, 265\n",
    "                # 401, 433, 435, 288, 367, 379, 401, 323, 366, 376, 454, 425, 426, 436, 427, \n",
    "                # 61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 78, 191, 80, 81, 82, \n",
    "                # 13, 312, 311, 310, 415, 308, 324, 318, 402, 317, 14, 87, 377, 152, 400, 148, 213, 214, 192, 138, 32, 74, 397, 365\n",
    "\n",
    "# MOUTH_INDICES += [i for i in range()]\n",
    "            # NOSE_INDICES = [2, 98, 327, 168]  # Nose bridge and tip\n",
    "            # NOSE_INDICES = [197]#, 5, 4, 1] \n",
    "\n",
    "# cv2.fillPoly(mask, [np.array(mask_points, np.int32)], (100, 100, 100))  # Black-out mask area\n",
    "            # for i in range(len(mask_points)):\n",
    "            #     cv2.putText(mask, str(frame_points[i]), mask_points[i], cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "            #     cv2.circle(mask, mask_points[i], 2, (0, 0, 255), -1)\n",
    "            # Apply mask\n",
    "\n",
    "# Create mask and fill region\n",
    "            # mask = np.ones_like(frame) * 255  # White mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "689c4d52-973e-45cb-b524-e466cd71e0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 1372, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cb42ea8-c243-4c11-a44e-729024e41109",
   "metadata": {},
   "outputs": [],
   "source": [
    "texture =cv2.imread('grass_texture.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c87aab13-880c-40f9-b279-4c9ec65d9f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 1372, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texture[0: mask.shape[0], 0:mask.shape[1], :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b30cf0-5dbf-463b-81c6-bf797e11fdb6",
   "metadata": {},
   "source": [
    "## Texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ffe03c83-6957-4d6b-b821-3f1563e2fb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740171015.454991   99504 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740171015.463138   99504 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Load image\n",
    "cap = cv2.imread('face.jpg')\n",
    "texture = cv2.imread('grass_texture.jpeg')\n",
    "\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    frame = cap\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            # Define the landmark indices for the mouth and nose region\n",
    "            MOUTH_INDICES = [\n",
    "                93, 137, 123, 50, 101, 100, 47, 114, 122, 188, 6, # left\n",
    "                351, 412, 343, 277, 329, 330, 280, 352, 366,323, # right\n",
    "            ]\n",
    "            NOSE_INDICES = []\n",
    "            \n",
    "            # Get landmark coordinates for mask area\n",
    "            h, w, _ = frame.shape\n",
    "            mask_points = []\n",
    "            for idx in MOUTH_INDICES + NOSE_INDICES:\n",
    "                x = int(landmarks.landmark[idx].x * w)\n",
    "                y = int(landmarks.landmark[idx].y * h)\n",
    "                mask_points.append((x, y))\n",
    "\n",
    "            left_ind = [132, 215, 172, 136, 150, 149, 176, 148, 152]  # left\n",
    "            right_ind = [377, 400, 378, 379, 365, 397, 288, 361]  # right\n",
    "\n",
    "            for idx in left_ind:\n",
    "                x = int(landmarks.landmark[idx].x * w)# - 15\n",
    "                y = int(landmarks.landmark[idx].y * h)# + 10\n",
    "                mask_points.append((x, y))\n",
    "\n",
    "            for idx in right_ind:\n",
    "                x = int(landmarks.landmark[idx].x * w)# + 15\n",
    "                y = int(landmarks.landmark[idx].y * h)# + 10\n",
    "                mask_points.append((x, y))\n",
    "\n",
    "            mask = frame\n",
    "            texture = cv2.resize(texture, (mask.shape[1], mask.shape[0]))  # Resize texture to match the mask size\n",
    "            # print(texture.shape)\n",
    "            hull = cv2.convexHull(np.array(mask_points, np.int32))\n",
    "\n",
    "            # Create a single-channel mask (grayscale) for the convex hull area\n",
    "            hull_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.uint8)  # Single-channel mask\n",
    "            cv2.fillConvexPoly(hull_mask, hull, 255)  # Fill the hull with white (255) in single-channel mask\n",
    "\n",
    "            # Apply texture using the hull mask (only inside the convex hull)\n",
    "            texture_in_hull = cv2.bitwise_and(texture, texture, mask=hull_mask)\n",
    "\n",
    "            # Keep the original image outside the hull and blend the texture inside the hull\n",
    "            mask = cv2.bitwise_and(mask, mask, mask=cv2.bitwise_not(hull_mask))  # Keep outside the hull\n",
    "            mask = cv2.add(mask, texture_in_hull)  # Add texture inside the hull\n",
    "\n",
    "            frame = mask\n",
    "\n",
    "    # Save the resulting image with texture inside the hull\n",
    "    cv2.imwrite(\"Masked_Image.jpg\", frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda97c30-a364-4eaf-ad4b-65ee1120c72a",
   "metadata": {},
   "source": [
    "## Half Face Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a98dbe61-e810-4fe5-9246-d755b13b984f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740273833.149691  243703 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740273833.160424  243710 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "### import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Load image\n",
    "# cap = cv2.imread(\"/projectnb/cs585bp/students/dlgirija/colorferet_extracted/00001/00001_930831_fa_a.ppm\")\n",
    "cap = cv2.imread('face.jpg')\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    frame = cap\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            # Define the landmark indices for the mouth and nose region\n",
    "            MOUTH_INDICES = [\n",
    "                227, 111, 229, 230,\n",
    "                447, 340, 449, 451,\n",
    "                93, 137, 123, 50, 101, 100, 47, 114, 122, 188, 6, # left\n",
    "                351, 412, 343, 277, 329, 330, 280, 352, 366,323, # right\n",
    "            ]\n",
    "            NOSE_INDICES = []\n",
    "            # Get landmark coordinates for mask area\n",
    "            h, w, _ = frame.shape\n",
    "            mask_points = []\n",
    "            frame_points = []\n",
    "            for idx in MOUTH_INDICES + NOSE_INDICES:\n",
    "                x = int(landmarks.landmark[idx].x * w)\n",
    "                y = int(landmarks.landmark[idx].y * h)\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "\n",
    "            left_ind = [132,215,172,136,150,149,176,148,152] # left\n",
    "            right_ind = [377,400,378,379,365,397,288,361] # right\n",
    "\n",
    "            for idx in left_ind:\n",
    "                x = int(landmarks.landmark[idx].x * w)# - 15\n",
    "                y = int(landmarks.landmark[idx].y * h)# + 10\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "\n",
    "            for idx in right_ind:\n",
    "                x = int(landmarks.landmark[idx].x * w) #+ 15\n",
    "                y = int(landmarks.landmark[idx].y * h) #+ 10\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "\n",
    "            mask = frame\n",
    "            hull = cv2.convexHull(np.array(mask_points, np.int32))\n",
    "            cv2.fillConvexPoly(mask, hull, (100, 100, 100))\n",
    "            frame = mask\n",
    "            \n",
    "    cv2.imwrite(\"Masked_Image.jpg\", frame)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e4fabf-b19d-4886-ab4e-4adf85d82d6c",
   "metadata": {},
   "source": [
    "## Full Face mask except EYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9998acfe-9a4c-47a6-9cc3-a282ad7644f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740276662.216682  245869 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740276662.229512  245869 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "### import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Load image\n",
    "# cap = cv2.imread(\"/projectnb/cs585bp/students/dlgirija/colorferet_extracted/00001/00001_930831_fa_a.ppm\")\n",
    "cap = cv2.imread('face.jpg')\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    frame = cap\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            # Define the landmark indices for the mouth and nose region\n",
    "            MOUTH_INDICES = [\n",
    "                227, 111, 229, 230,\n",
    "                447, 340, 449, 451,\n",
    "                93, 137, 123, 50, 101, 100, 47, 114, 122, 188, 6, # left\n",
    "                351, 412, 343, 277, 329, 330, 280, 352, 366,323, # right\n",
    "            ]\n",
    "            NOSE_INDICES = []\n",
    "            # Get landmark coordinates for mask area\n",
    "            h, w, _ = frame.shape\n",
    "            mask_points = []\n",
    "            frame_points = []\n",
    "            for idx in MOUTH_INDICES + NOSE_INDICES:\n",
    "                x = int(landmarks.landmark[idx].x * w)\n",
    "                y = int(landmarks.landmark[idx].y * h)\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "\n",
    "            left_ind = [132,215,172,136,150,149,176,148,152] # left\n",
    "            right_ind = [377,400,378,379,365,397,288,361] # right\n",
    "\n",
    "            for idx in left_ind:\n",
    "                x = int(landmarks.landmark[idx].x * w)# - 15\n",
    "                y = int(landmarks.landmark[idx].y * h)# + 10\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "\n",
    "            for idx in right_ind:\n",
    "                x = int(landmarks.landmark[idx].x * w) #+ 15\n",
    "                y = int(landmarks.landmark[idx].y * h) #+ 10\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "\n",
    "            mask = frame\n",
    "            hull = cv2.convexHull(np.array(mask_points, np.int32))\n",
    "            cv2.fillConvexPoly(mask, hull, (100, 100, 100))\n",
    "            frame = mask\n",
    "\n",
    "            EYE_INDICES = [\n",
    "                21, 54, 103, 67, 109, 10, 297, 332, 284, 251, 389, 338,\n",
    "                68, 104, 69, 151, 299, 298, 301, 337, 333, 69, 71, 162, 108\n",
    "            ]\n",
    "            \n",
    "            mask_points = []\n",
    "            frame_points = []\n",
    "            for idx in EYE_INDICES:\n",
    "                x = int(landmarks.landmark[idx].x * w)# - 15\n",
    "                y = int(landmarks.landmark[idx].y * h)# + 10\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "                \n",
    "            mask = frame\n",
    "            hull = cv2.convexHull(np.array(mask_points, np.int32))\n",
    "            cv2.fillConvexPoly(mask, hull, (100, 100, 100))\n",
    "            frame = mask\n",
    "            cv2.fillPoly(mask, [np.array(mask_points, np.int32)], (100, 100, 100))  # Black-out mask area\n",
    "\n",
    "            EYE_SIDE_INDICES = [\n",
    "                162, 127, 234, 227,\n",
    "                14, 156, 141, 139, 70, 71\n",
    "                # 21, 54, 103, 67, 109, 10, 297, 332, 284, 251, 389, 338,\n",
    "                # 68, 104, 69, 151, 299, 298, 301, 337, 333, 69, 71, 162, 108\n",
    "            ]\n",
    "            \n",
    "            mask_points = []\n",
    "            frame_points = []\n",
    "            for idx in EYE_SIDE_INDICES:\n",
    "                x = int(landmarks.landmark[idx].x * w)# - 15\n",
    "                y = int(landmarks.landmark[idx].y * h)# + 10\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "                \n",
    "            mask = frame\n",
    "            hull = cv2.convexHull(np.array(mask_points, np.int32))\n",
    "            cv2.fillConvexPoly(mask, hull, (100, 100, 100))\n",
    "            frame = mask\n",
    "            cv2.fillPoly(mask, [np.array(mask_points, np.int32)], (100, 100, 100))  # Black-out mask area\n",
    "\n",
    "            EYE_SIDE_INDICES_RIGHT = [\n",
    "                300, 383, 372, 389, 298,\n",
    "                356, 368, 447, 261, 449\n",
    "                # 162, 127, 234, 227,\n",
    "                # 14, 156, 141, 139, 70, 71\n",
    "                # 21, 54, 103, 67, 109, 10, 297, 332, 284, 251, 389, 338,\n",
    "                # 68, 104, 69, 151, 299, 298, 301, 337, 333, 69, 71, 162, 108\n",
    "            ]\n",
    "            \n",
    "            mask_points = []\n",
    "            frame_points = []\n",
    "            for idx in EYE_SIDE_INDICES_RIGHT:\n",
    "                x = int(landmarks.landmark[idx].x * w)# - 15\n",
    "                y = int(landmarks.landmark[idx].y * h)# + 10\n",
    "                mask_points.append((x, y))\n",
    "                frame_points.append(idx)\n",
    "                \n",
    "            mask = frame\n",
    "            hull = cv2.convexHull(np.array(mask_points, np.int32))\n",
    "            cv2.fillConvexPoly(mask, hull, (100, 100, 100))\n",
    "            frame = mask\n",
    "            cv2.fillPoly(mask, [np.array(mask_points, np.int32)], (100, 100, 100))  # Black-out mask area\n",
    "            # for i in range(len(mask_points)):\n",
    "            #     cv2.putText(mask, str(frame_points[i]), mask_points[i], cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "            #     cv2.circle(mask, mask_points[i], 2, (0, 0, 255), -1)\n",
    "            # Apply mask\n",
    "\n",
    "    \n",
    "    cv2.imwrite(\"Masked_Image.jpg\", frame)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa24ec71-7a0c-4dbe-8385-34f1a9fc8366",
   "metadata": {},
   "source": [
    "## Blurred Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b61f9580-31de-4762-ad16-eba2a6e0ca06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.imread('face.jpg')\n",
    "kernel = np.ones((5,5),np.float32)/25\n",
    "cap = cv2.filter2D(cap,-1,kernel)\n",
    "cv2.imwrite(\"Masked_Image.jpg\", cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23676bf3-fad4-45c2-ae24-94880c4aba41",
   "metadata": {},
   "source": [
    "## Random parts removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bbc49bb7-3aac-4005-b260-7e84bb44d743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n",
      "{1, 2, 5, 6, 7, 8, 10, 11, 15, 16, 17, 18, 20, 21, 24, 25, 26, 29, 34, 35, 40, 41, 42, 48, 52, 53, 56, 57, 59, 60, 61, 63, 64, 65, 67, 71, 72, 76, 77, 81, 83, 86, 87, 89, 90, 92, 97, 99, 101, 103, 104, 106, 110, 111, 114, 122, 124, 125, 126, 129, 132, 133, 134, 135, 139, 141, 142, 143, 144, 146, 147, 148, 149, 151, 152, 153, 155, 156, 157, 158, 162, 163, 165, 166, 167, 170, 174, 175, 176, 177, 178, 180, 184, 187, 188, 191, 192, 193, 196, 197, 203, 206, 207, 209, 210, 214, 216, 219, 220, 221, 222, 223, 224, 225, 228, 229, 231, 234, 237, 238, 239, 245, 247, 249, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 266, 270, 271, 272, 274, 277, 279, 280, 283, 285, 288, 290, 293, 295, 296, 297, 299, 300, 301, 303, 305, 306, 307, 309, 310, 312, 313, 314, 316, 317, 318, 321, 326, 333, 334, 335, 336, 337, 338, 339, 341, 342, 343, 344, 345, 346, 347, 351, 352, 355, 356, 360, 362, 363, 364, 365, 366, 368, 370, 371, 373, 375, 376, 377, 380, 382, 385, 386, 388, 390, 391, 392, 396, 399, 401, 406, 410, 411, 412, 415, 416, 417, 419, 420, 424, 428, 431, 434, 436, 437, 439, 441, 443, 444, 445, 446, 448, 449, 450, 451, 452, 454, 455, 456, 458, 460, 462, 464, 465, 467}\n",
      "[(681, 721), (681, 738), (682, 660), (683, 566), (518, 534), (684, 508), (685, 327), (679, 804), (680, 834), (680, 848), (680, 861), (681, 877), (660, 724), (432, 420), (541, 560), (510, 543), (606, 556), (529, 507), (424, 531), (464, 540), (609, 797), (637, 808), (623, 808), (610, 699), (533, 477), (500, 479), (599, 514), (564, 804), (626, 713), (645, 722), (589, 807), (487, 460), (609, 707), (577, 483), (545, 335), (447, 448), (654, 800), (594, 807), (603, 814), (642, 808), (651, 875), (656, 830), (658, 821), (619, 816), (614, 823), (589, 767), (653, 734), (649, 729), (555, 639), (488, 353), (507, 401), (599, 846), (521, 554), (467, 576), (630, 597), (661, 571), (469, 511), (669, 729), (616, 639), (604, 691), (438, 690), (616, 541), (641, 666), (505, 832), (435, 485), (672, 731), (597, 655), (445, 540), (543, 544), (598, 818), (456, 692), (637, 952), (571, 913), (685, 398), (683, 958), (583, 546), (612, 543), (452, 502), (596, 520), (575, 510), (421, 463), (528, 539), (608, 755), (625, 711), (650, 754), (563, 895), (645, 612), (683, 948), (601, 935), (438, 692), (639, 817), (631, 834), (602, 805), (486, 713), (646, 582), (614, 803), (486, 767), (654, 535), (661, 602), (683, 598), (585, 709), (565, 732), (516, 738), (617, 661), (540, 847), (513, 800), (548, 763), (616, 708), (639, 694), (616, 514), (578, 502), (545, 498), (518, 498), (499, 503), (505, 572), (530, 582), (590, 578), (423, 573), (646, 713), (658, 723), (646, 714), (644, 561), (503, 518), (852, 539), (942, 429), (782, 564), (805, 566), (828, 565), (763, 559), (818, 509), (792, 510), (841, 512), (856, 518), (878, 566), (767, 906), (861, 532), (947, 540), (796, 679), (751, 801), (722, 811), (737, 812), (700, 720), (752, 614), (755, 688), (865, 672), (871, 484), (732, 504), (913, 761), (717, 725), (885, 465), (791, 486), (796, 464), (826, 340), (808, 400), (908, 482), (925, 456), (727, 805), (731, 721), (766, 813), (757, 819), (725, 714), (733, 809), (700, 812), (711, 877), (708, 859), (704, 832), (702, 822), (738, 817), (751, 835), (709, 736), (865, 406), (846, 458), (763, 850), (741, 471), (751, 398), (762, 330), (848, 559), (751, 554), (881, 522), (737, 599), (740, 698), (927, 597), (885, 605), (854, 618), (706, 572), (919, 652), (749, 642), (953, 527), (742, 679), (752, 544), (723, 668), (862, 839), (860, 856), (941, 646), (937, 493), (690, 732), (769, 659), (827, 549), (763, 823), (912, 701), (730, 953), (786, 550), (757, 545), (795, 513), (817, 512), (850, 523), (842, 545), (754, 759), (737, 714), (732, 944), (721, 613), (931, 701), (740, 865), (789, 790), (880, 722), (721, 584), (747, 808), (880, 775), (714, 536), (705, 603), (736, 653), (783, 865), (729, 926), (796, 880), (851, 808), (815, 770), (735, 627), (747, 712), (752, 516), (824, 502), (852, 503), (871, 509), (884, 541), (864, 578), (839, 588), (808, 588), (779, 581), (757, 574), (948, 581), (746, 717), (720, 640), (703, 724), (739, 723), (697, 730), (730, 556), (723, 563), (867, 524)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1740280183.345630  247968 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740280183.356214  247968 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "### import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialize MediaPipe FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Load image\n",
    "# cap = cv2.imread(\"/projectnb/cs585bp/students/dlgirija/colorferet_extracted/00001/00001_930831_fa_a.ppm\")\n",
    "cap = cv2.imread('face.jpg')\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    frame = cap\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        # for landmarks in results.multi_face_landmarks:\n",
    "        inds = [i for i in range(0, 468)]\n",
    "        num = random.randint(0, 467)\n",
    "        INDICES = set()\n",
    "        for i in range(num):\n",
    "            INDICES.add(inds[random.randint(0, 467)])\n",
    "        # Get landmark coordinates for mask area\n",
    "        h, w, _ = frame.shape\n",
    "        mask_points = []\n",
    "        frame_points = []\n",
    "        for idx in INDICES:\n",
    "            x = int(landmarks.landmark[idx].x * w)\n",
    "            y = int(landmarks.landmark[idx].y * h)\n",
    "            mask_points.append((x, y))\n",
    "            frame_points.append(idx)\n",
    "\n",
    "        mask = frame\n",
    "        cv2.fillPoly(mask, [np.array(mask_points, np.int32)], (100, 100, 100))  # Black-out mask area\n",
    "        \n",
    "        # hull = cv2.convexHull(np.array(mask_points, np.int32))\n",
    "        # cv2.fillConvexPoly(mask, hull, (100, 100, 100))\n",
    "        frame = mask\n",
    "\n",
    "        print(num)\n",
    "        print(INDICES)\n",
    "        print(mask_points)\n",
    "        \n",
    "            # for i in range(len(mask_points)):\n",
    "            #     cv2.putText(mask, str(frame_points[i]), mask_points[i], cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "            #     cv2.circle(mask, mask_points[i], 2, (0, 0, 255), -1)\n",
    "            # Apply mask\n",
    "\n",
    "    \n",
    "    cv2.imwrite(\"Masked_Image.jpg\", frame)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d070463-a28b-401b-83ad-5dab8ab2c52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
